title: "科学探索也能被机器取代吗？"
date: 2018-01-18 20:38:52 +0800
author: huaxia
cover: "-/images/automated-science/1.webp"
tags:
    - 算法
    - 科学哲学
    - 人工智能
preview: '弗朗西斯·培根 洞悉了这样一个事实：科学发现的过程本质上是算法。'
editor: "于翮,子川"
proof-reader: "付安琪,武权"
design: "童画,姜如月"

---

![](-/images/automated-science/1.webp)


“志在学习真理的人，在研读科学家的著作时，当如入敌阵，发起全面的诘攻。为了不当刻薄者，也不当老好人，审视书本时，他应时刻保持自省。”

——海什木（Ibn al-Haytham）(公元 965 - 1040 年）

# 1

科学正陷入数据危机之中。2016年，生物医学领域发布了一百二十多万篇新论文，经过同行审议的生物医学论文总数也超过2600万。但平均而言，一个科学家每年能读完的只有大概250篇。科研论文的质量已经在下降了。最近有很多研究表明，其中大部分生物医学论文的结论都无法复现（译者注：一般而言成功的科学实验是可以重复的，不同的实验者在前提一致，操作步骤一致的情况下，能够得到相同的结果）。

数据太多，质量太低——这两个问题源于人脑有限的处理能力。科学家构建假说所依赖的知识越来越片面和局限，他们不是提出越来越多错误的问题，就是在问许多已经被解决的问题。人们的创造行为日益变成过去经验的随机组合，以致于只有具备特定经验的科学家，才能注意到别人忽视的事件。虽然一直以来，运气都在科学发现中占有一席之地，但目前它所占的比例实在太高了。

对科学探索而言，有一种方式能够克服当前的危机：将计算机与人工智能结合。比之人脑，计算机的存储能力和计算能力都更强。科学的自动化将极大加速科学发展的速度，甚至会带来一场新的科学革命。但这背后隐藏着一个更大的问题：科学发现真的能被自动化吗？

我相信这是可能的，17世纪英国哲学家和科学先驱——弗朗西斯·培根，早就在他的作品里回答了这个问题。

![](-/images/automated-science/2.webp)
<center><small>Science Fiction Jimmy Ernst ( 1948 )</small></center>


早在培根之前的几个世纪，以海什木（Ibn al-Haytham）为代表的穆斯林思想家们，就论述过这种强调经验主义和实验方法的科学观。然而是培根将其正式确定，并使之成为一种学说的。

在他的《新工具》（Novum Organum）中，他提出了科学发现的模式，即所谓培根方法（Baconian method）。他反对在构建科学理论时使用三段论，这种方法在他看来并不可靠。反之，他提出对现象要系统、周密地观察，并以归纳法进行客观分析，以产生可推广的结论。在他看来，只有排除不完全的前提假设，才能发现真理。

培根方法将构建科学基础和完善科学理论的过程分别勾勒，以去除观察和概念构建过程中的逻辑偏见。他的想法是，让观察者大量搜集关于自然现象的信息，并将其归为一个统一架构，以便归纳分析。在《新工具》中，他写到：“经验主义者像蚂蚁，不断囤积（知识)，不断使用。而理性主义者像蜘蛛，不断编织（理论）。但最好是做蜜蜂，取二者之间，归纳整理现有材料并拿来使用。”

因为费时费力，培根方法现今很少被使用，它的技术应用也不太明朗。然而在当时，这种想法极富革命性。因为在此之前，科学研究是形而上学，属于出身贵族的少数学者的特权。但培根拒绝了古希腊思想的权威，他描绘了科学发现的步骤，他的蓝图使得任何背景的人都有可能成为科学家。

培根的洞察也揭示了一个潜在事实：科学发现的过程本质上是算法——重复执行有限步骤，直到发现有价值的结论。培根明确使用了“机械化(machine)” 这个词来描述他的想法。他的科学发现算法有三个组成部分：

> 1. 所有搜集到的观察要统一在一个知识体（corpus of knowledge）里；
> 2. 新的观察可以产生新的假说；
> 3. 所有假说需要通过精心设计的实验验证。

![](-/images/automated-science/3.webp)
<center><small>see-see-rider Jimmy Ernst（1946）</small></center>

# 2
如果科学本质是算法，那么它一定有被自动化的可能。这个“幻梦”困扰了计算机专家数十年，主要是因为科学发现涉及三个层面：感官观察现象，心智构建假说，机械化地实验。而科学自动化需要嵌入每个层面，并使得三者互相对接而不产生摩擦。目前还没有算法能结合这三个方面。

不过，实验层面最近进展显著。比如，制药业通常使用**自动化高吞吐量平台**进行药物设计。加州的初创企业Transcriptic 和 Emerald Cloud Lab 建立的系统，能自动完成生物医学专家要做的大多数体力活。科学家上传实验方案，实验方案随即被转化为代码，输入机器人平台，以进行一系列生物学实验操作。这些解决方案尤其适用于那些需要进行密集实验的学科，比如分子生物学和化学工程。类似方法同样可应用在其他重数据的领域，甚至能拓展到理论研究中。

另一方面，自动构造假说则相对落后，但 Don Swanson 在上世纪八十年代的工作取得了一些突破。他证明了某些不相关的科学思想间有潜在的关联，通过用简单的逻辑演绎，他能将来自不同领域且互不交叉的文章联系起来。比如，无需经过任何实验，或者必须精通哪个领域，他就能预判瘦身鱼油和雷诺氏综合征（Reynaud’s Syndrome）之间存在某种联系。

最近的一些研究，比如Andrey Rzhetsky 在芝加哥大学的工作和 Albert-László Barabási 在东北大学（Northeastern U）的工作，他们在数学模型和图论的基础上，将已有知识数据库映射为一个网络，节点代表概念，联接代表概念间的关系。这样，未知的联接节点可能代表新的假说。

![](-/images/automated-science/4.webp)
<center><small>night subway Jimmy Ernst（1948）</small></center>
科学自动化里的最大难题在于，如何大规模搜集可靠的科学观察。目前在观测层面上，还没有一个统一的数据库储存着人类的所有科学知识。尽管自然语言处理技术已经大大发展，不仅能够挖据科研论文之间的关系，还能获取它们的背景资料。但几家主要科学出版商对挖掘他们的出版物文本有严格限制。更大的问题是，论文不但总会被科学家自身的偏见（或概念误用）所影响，也包含了大量复合概念与方法论，难以被提取和量化。

虽然困难重重，但最近计算科学和网络数据库的发展，让培根方法有史以来第一次变得实用。即使科学发现不能完全自动化，一旦还原论的使用达到极限，培根方法也会变得重要起来。

**在这个数据量愈发庞大的时代，人类心智无法重建极其复杂的自然现象。现代的培根方法结合了还原论和数据挖掘技术，并通过基于归纳的计算模型来分析信息，使得我们对自然界的理解能够得到提升。这种方法能自动地产生一些很有前景的假说并进行验证，以此来加速我们的知识迭代**。

**它也提供了一个科学探索应有的模样：追寻真理，不畏权威，崇尚自由**。

-----

原文链接：[Science has outgrown the human mind and its limited capacities | Aeon](https://aeon.co/ideas/science-has-outgrown-the-human-mind-and-its-limited-capacities)